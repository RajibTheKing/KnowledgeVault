### 13:07
	- start working with GPT based model, for medical NER task
- ### 13:40
	- setup a new python environment for specifically GPT solution
	- learn how to use GPT based transformer model to pre-train
	- using this one https://huggingface.co/openai-community/gpt2
- ### 13:48
	- implement training and pre-processing for GPT2
	- try to move the training device to GPU
- ### 14:18
	- training is done (it took 18 minutes for 500 annotated samples on my Computer, GPU-RTX4070ti)
	- fine tuned GPT model is stored into the disk
- ### 14:32
	- it's time to write the implementations for prediction based on unseen medical texts
	-